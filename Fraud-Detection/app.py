<<<<<<< HEAD:Fraud-Detection/src/app.py
import os
import pickle
import pandas as pd
import seaborn as sns
import streamlit as st
import matplotlib.pyplot as plt

# Load the trained ML model and the scaler object
# Returns the model and scaler if they exist, otherwise returns None
# Caching model and scaler for performance

@st.cache_resource
def load_model_and_scaler():
    model_path = "models/fraud_model.pkl"
    scaler_path = "models/scaler.pkl" 
    if not os.path.exists(model_path) or not os.path.exists(scaler_path):
        return None, None
    with open(model_path, "rb") as f:
        model = pickle.load(f)
    with open(scaler_path, "rb") as f:
        scaler = pickle.load(f)
    return model, scaler

# Preprocess the dataset: fill missing values, remove outliers, encode categorical columns
# Caching preprocessing of uploaded dataset

@st.cache_data
def preprocess_dataset(dataset):
    # Fill missing values in 'Amount' column with the mean
    dataset["Amount"] = dataset["Amount"].fillna(dataset["Amount"].mean())

    # Drop ID-related columns if they exist (not useful for prediction)
    dataset.drop(columns=["TransactionID", "CustomerID"], inplace=True, errors='ignore')

    # Remove outliers from numeric columns using the IQR method
    numeric_columns = ["Amount", "TransactionSpeed"] if "TransactionSpeed" in dataset.columns else ["Amount"]
    for col in numeric_columns:
        Q1 = dataset[col].quantile(0.25)
        Q3 = dataset[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        dataset = dataset[(dataset[col] >= lower_bound) & (dataset[col] <= upper_bound)]

    # Convert categorical variables into dummy/one-hot encoded columns
    categorical_columns = ["TransactionType", "Location", "DeviceType", "TimeOfDay"]
    existing_cols = [col for col in categorical_columns if col in dataset.columns]
    dataset = pd.get_dummies(dataset, columns=existing_cols, drop_first=True)

    return dataset

# Main Streamlit web application

def streamlit_app():
    st.set_page_config(page_title="Credit Card Fraud Detection", page_icon="ðŸ’³", layout="wide")
    st.sidebar.title("ðŸ” Navigation")
    page = st.sidebar.radio("Go to:", ["ðŸ  Home", "ðŸ“¥ Upload & Preprocess", "ðŸ“Š Visualization", "ðŸ§  Model Prediction"])

    # ----------------------------- HOME PAGE ----------------------------- #
    if page == "ðŸ  Home":
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            st.image("assets/fraud_banner.jpg", use_container_width=True)
        st.title("ðŸ›¡ï¸ Credit Card Fraud Detection System")
        st.markdown("Predict the likelihood of fraud in transaction data using a trained machine learning model.")

        col1, col2 = st.columns(2)
        with col1:
            st.markdown("### âœ… What You Can Do:")
            st.markdown("""
                ðŸ”¹ Upload your transaction CSV file  
                ðŸ”¹ Preprocess and clean the data  
                ðŸ”¹ Visualize fraud patterns  
                ðŸ”¹ Predict fraud using ML  
                ðŸ”¹ Download results with probabilities  
            """)

        with col2:
            st.markdown("### ðŸ§­ How to Use:")
            st.markdown("""
                ðŸ”¹ Upload a CSV file  
                ðŸ”¹ Go to 'Upload & Preprocess Data'  
                ðŸ”¹ Explore 'Visualization'  
                ðŸ”¹ Use 'Model Prediction'  
                ðŸ”¹ Download your fraud results  
            """)

        st.markdown("---")
        st.markdown("Made with :sparkling_heart: using Python, Streamlit, Scikit-learn  \nBy **Shambhuraj Patil**")

    # ----------------------- UPLOAD & PREPROCESS ------------------------ #
    elif page == "ðŸ“¥ Upload & Preprocess":
        st.title("ðŸ“¥ Upload & Preprocess Data")

        with st.expander("ðŸ“ Please read this before uploading your file (Important)", expanded=False):
            st.markdown("""
            - Make sure your CSV file contains the following required columns:  
              `TransactionID`, `CustomerID`, `Amount`, `TransactionType`, `Location`, `DeviceType`, `TimeOfDay`, `Fraud`
            - Missing or renamed columns will prevent the app from working properly.
            """)

        # Offer sample CSV download for user reference
        try:
            sample_dataset = pd.read_csv("data/fraud_detection.csv").head(20)
            st.download_button("ðŸ“„ Download Sample Input CSV", sample_dataset.to_csv(index=False),
                            file_name="sample_input.csv", mime="text/csv")
        except FileNotFoundError:
            st.warning("Sample CSV not found. Please check file path or re-upload.")

        upload_file = st.file_uploader("ðŸ“¤ Upload a CSV file", type="csv")

        if upload_file:
            dataset = pd.read_csv(upload_file)
            st.session_state["raw_data"] = dataset.copy()  # Save raw data for visualization

            # Validate column presence
            required_columns = ["TransactionID", "CustomerID", "Amount", "TransactionType",
                                "Location", "DeviceType", "TimeOfDay", "Fraud"]

            missing_cols = [col for col in required_columns if col not in dataset.columns]
            if missing_cols:
                st.error(f"âŒ Your dataset is missing required columns: {missing_cols}")
                st.markdown("ðŸ“Œ Please ensure your file has all required columns. You can download a sample format above.")
                st.stop()

            st.success("âœ… File Uploaded Successfully")
            st.subheader("ðŸ“„ Raw Data Preview")
            st.dataframe(dataset.head(10))

            # Apply preprocessing steps with spinner
            with st.spinner("â³ Processing your data..."):
                dataset = preprocess_dataset(dataset)
                st.session_state["preprocessed_data"] = dataset

            st.subheader("âœ… Preprocessed Data")
            st.dataframe(dataset.head(10))

    # ---------------------------- VISUALIZATION -------------------------- #
    elif page == "ðŸ“Š Visualization":
        st.title("ðŸ“Š Fraud Data Visualization")

        if "preprocessed_data" not in st.session_state:
            st.warning("âš ï¸ Please upload and preprocess your data first.")
            st.stop()

        dataset = st.session_state["raw_data"]  # Use raw data to retain original categorical columns

        tab1, tab2, tab3, = st.tabs(["Fraud vs Non-Fraud", "Fraud by Transaction Type", "Amount"])

        with tab1:
            st.subheader("ðŸ“‰ Fraud vs Non-Fraud Distribution")
            fig1, ax1 = plt.subplots()
            sns.countplot(x="Fraud", hue="Fraud", data=dataset, palette=["green", "red"], ax=ax1)
            st.pyplot(fig1)

        with tab2:
            if "TransactionType" in dataset.columns:
                st.subheader("ðŸ’³ Fraud Distribution by Transaction Type")
                fig2, ax2 = plt.subplots()
                sns.countplot(x="TransactionType", hue="Fraud", data=dataset, palette=["green", "red"], ax=ax2)
                plt.xticks(rotation=45)
                st.pyplot(fig2)
            else:
                st.info("'TransactionType' column not found in your dataset.")

        with tab3:
            if "Amount" in dataset.columns:
                st.subheader("ðŸ’° Amount Distribution")
                fig3, ax3 = plt.subplots()
                sns.histplot(data=dataset, x="Amount", hue="Fraud", bins=40, kde=True, ax=ax3)
                st.pyplot(fig3)

    # -------------------------- MODEL PREDICTION -------------------------- #
    elif page == "ðŸ§  Model Prediction":
        st.title("ðŸ§  Model Prediction")

        if "preprocessed_data" not in st.session_state:
            st.warning("âš ï¸ Please upload and preprocess your data first.")
            st.stop()

        dataset = st.session_state["preprocessed_data"]

        st.markdown("### ðŸ§¾ Input Format Notice")
        st.info(
            """âš ï¸ **Important:**  
            - Your preprocessed dataset must match the model's trained feature structure.  
            - Unknown categories or missing dummy columns will be automatically filled with zeros.  
            - Make sure preprocessing steps like encoding and outlier removal were applied."""
        )

        model, scaler = load_model_and_scaler()
        if model is None or scaler is None:
            st.error("ðŸš« Model or Scaler files not found. Make sure 'fraud_model.pkl' and 'scaler.pkl' are in the working directory.")
            st.stop()

        # Align input dataset with model features
        model_features = model.feature_names_in_
        for col in model_features:
            if col not in dataset.columns:
                dataset[col] = 0  # Add missing columns as 0
        dataset = dataset[model_features]  # Reorder columns

        # Apply scaling and prediction
        scaled_data = scaler.transform(dataset)
        predictions = model.predict(scaled_data)
        probabilities = model.predict_proba(scaled_data)[:, 1] * 100

        # Attach results to the dataset
        result_dataset = dataset.copy()
        result_dataset["Fraud Prediction"] = predictions.astype(int)
        result_dataset["Fraud Probability (%)"] = probabilities.round(2)

        st.subheader("ðŸ” Prediction Results")

        # Add checkbox to control how many rows to show
        if st.checkbox("Show all prediction results", value=False):
            st.dataframe(result_dataset[["Fraud Prediction", "Fraud Probability (%)"]])
        else:
            st.dataframe(result_dataset[["Fraud Prediction", "Fraud Probability (%)"]].head(20))
        
        st.markdown("---")
        st.download_button("Download Predictions", result_dataset.to_csv(index=False), file_name="fraud_predictions.csv", mime="text/csv")
        st.success(f"âœ… Total transactions analyzed: {len(result_dataset)}")
        st.info(f"ðŸš¨ Fraudulent transactions detected: {(result_dataset['Fraud Prediction'] == 1).sum()}")

# Run the app
if __name__ == "__main__":
    streamlit_app()
=======
import os
import pickle
import pandas as pd
import seaborn as sns
import streamlit as st
import matplotlib.pyplot as plt

# Load the trained ML model and the scaler object
# Returns the model and scaler if they exist, otherwise returns None
# Caching model and scaler for performance

@st.cache_resource
def load_model_and_scaler():
    model_path = "models/fraud_model.pkl"
    scaler_path = "models/scaler.pkl"
    if not os.path.exists(model_path) or not os.path.exists(scaler_path):
        return None, None
    with open(model_path, "rb") as f:
        model = pickle.load(f)
    with open(scaler_path, "rb") as f:
        scaler = pickle.load(f)
    return model, scaler

# Preprocess the dataset: fill missing values, remove outliers, encode categorical columns
# Caching preprocessing of uploaded dataset

@st.cache_data
def preprocess_dataset(dataset):
    # Fill missing values in 'Amount' column with the mean
    dataset["Amount"] = dataset["Amount"].fillna(dataset["Amount"].mean())

    # Drop ID-related columns if they exist (not useful for prediction)
    dataset.drop(columns=["TransactionID", "CustomerID"], inplace=True, errors='ignore')

    # Remove outliers from numeric columns using the IQR method
    numeric_columns = ["Amount", "TransactionSpeed"] if "TransactionSpeed" in dataset.columns else ["Amount"]
    for col in numeric_columns:
        Q1 = dataset[col].quantile(0.25)
        Q3 = dataset[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        dataset = dataset[(dataset[col] >= lower_bound) & (dataset[col] <= upper_bound)]

    # Convert categorical variables into dummy/one-hot encoded columns
    categorical_columns = ["TransactionType", "Location", "DeviceType", "TimeOfDay"]
    existing_cols = [col for col in categorical_columns if col in dataset.columns]
    dataset = pd.get_dummies(dataset, columns=existing_cols, drop_first=True)

    return dataset

# Main Streamlit web application

def streamlit_app():
    st.set_page_config(page_title="Credit Card Fraud Detection", page_icon="ðŸ’³", layout="wide")
    st.sidebar.title("ðŸ” Navigation")
    page = st.sidebar.radio("Go to:", ["ðŸ  Home", "ðŸ“¥ Upload & Preprocess", "ðŸ“Š Visualization", "ðŸ§  Model Prediction"])

    # ----------------------------- HOME PAGE ----------------------------- #
    if page == "ðŸ  Home":
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            st.image("assets/fraud_banner.jpg", use_container_width=True)

        st.title("ðŸ›¡ï¸ Credit Card Fraud Detection System")
        st.markdown("Predict the likelihood of fraud in transaction data using a trained machine learning model.")

        col1, col2 = st.columns(2)
        with col1:
            st.markdown("### âœ… What You Can Do:")
            st.markdown("""
                ðŸ”¹ Upload your transaction CSV file  
                ðŸ”¹ Preprocess and clean the data  
                ðŸ”¹ Visualize fraud patterns  
                ðŸ”¹ Predict fraud using ML  
                ðŸ”¹ Download results with probabilities  
            """)

        with col2:
            st.markdown("### ðŸ§­ How to Use:")
            st.markdown("""
                ðŸ”¹ Upload a CSV file  
                ðŸ”¹ Go to 'Upload & Preprocess Data'  
                ðŸ”¹ Explore 'Visualization'  
                ðŸ”¹ Use 'Model Prediction'  
                ðŸ”¹ Download your fraud results  
            """)

        st.markdown("---")
        st.markdown("Made with :sparkling_heart: using Python, Streamlit, Scikit-learn  \nBy **Shambhuraj Patil**")

    # ----------------------- UPLOAD & PREPROCESS ------------------------ #
    elif page == "ðŸ“¥ Upload & Preprocess":
        st.title("ðŸ“¥ Upload & Preprocess Data")

        with st.expander("ðŸ“ Please read this before uploading your file (Important)", expanded=False):
            st.markdown("""
            - Make sure your CSV file contains the following required columns:  
              `TransactionID`, `CustomerID`, `Amount`, `TransactionType`, `Location`, `DeviceType`, `TimeOfDay`, `Fraud`
            - Missing or renamed columns will prevent the app from working properly.
            """)

        # Offer sample CSV download for user reference
        try:
            sample_dataset = pd.read_csv("data/fraud_detection.csv").head(20)
            st.download_button("ðŸ“„ Download Sample Input CSV", sample_dataset.to_csv(index=False),
                            file_name="sample_input.csv", mime="text/csv")
        except FileNotFoundError:
            st.warning("Sample CSV not found. Please check file path or re-upload.")

        upload_file = st.file_uploader("ðŸ“¤ Upload a CSV file", type="csv")

        if upload_file:
            dataset = pd.read_csv(upload_file)
            st.session_state["raw_data"] = dataset.copy()  # Save raw data for visualization

            # Validate column presence
            required_columns = ["TransactionID", "CustomerID", "Amount", "TransactionType",
                                "Location", "DeviceType", "TimeOfDay", "Fraud"]

            missing_cols = [col for col in required_columns if col not in dataset.columns]
            if missing_cols:
                st.error(f"âŒ Your dataset is missing required columns: {missing_cols}")
                st.markdown("ðŸ“Œ Please ensure your file has all required columns. You can download a sample format above.")
                st.stop()

            st.success("âœ… File Uploaded Successfully")
            st.subheader("ðŸ“„ Raw Data Preview")
            st.dataframe(dataset.head(10))

            # Apply preprocessing steps with spinner
            with st.spinner("â³ Processing your data..."):
                dataset = preprocess_dataset(dataset)
                st.session_state["preprocessed_data"] = dataset

            st.subheader("âœ… Preprocessed Data")
            st.dataframe(dataset.head(10))

    # ---------------------------- VISUALIZATION -------------------------- #
    elif page == "ðŸ“Š Visualization":
        st.title("ðŸ“Š Fraud Data Visualization")

        if "preprocessed_data" not in st.session_state:
            st.warning("âš ï¸ Please upload and preprocess your data first.")
            st.stop()

        dataset = st.session_state["raw_data"]  # Use raw data to retain original categorical columns

        tab1, tab2, tab3, = st.tabs(["Fraud vs Non-Fraud", "Fraud by Transaction Type", "Amount"])

        with tab1:
            st.subheader("ðŸ“‰ Fraud vs Non-Fraud Distribution")
            fig1, ax1 = plt.subplots()
            sns.countplot(x="Fraud", hue="Fraud", data=dataset, palette=["green", "red"], ax=ax1)
            st.pyplot(fig1)

        with tab2:
            if "TransactionType" in dataset.columns:
                st.subheader("ðŸ’³ Fraud Distribution by Transaction Type")
                fig2, ax2 = plt.subplots()
                sns.countplot(x="TransactionType", hue="Fraud", data=dataset, palette=["green", "red"], ax=ax2)
                plt.xticks(rotation=45)
                st.pyplot(fig2)
            else:
                st.info("'TransactionType' column not found in your dataset.")

        with tab3:
            if "Amount" in dataset.columns:
                st.subheader("ðŸ’° Amount Distribution")
                fig3, ax3 = plt.subplots()
                sns.histplot(data=dataset, x="Amount", hue="Fraud", bins=40, kde=True, ax=ax3)
                st.pyplot(fig3)

    # -------------------------- MODEL PREDICTION -------------------------- #
    elif page == "ðŸ§  Model Prediction":
        st.title("ðŸ§  Model Prediction")

        if "preprocessed_data" not in st.session_state:
            st.warning("âš ï¸ Please upload and preprocess your data first.")
            st.stop()

        dataset = st.session_state["preprocessed_data"]

        st.markdown("### ðŸ§¾ Input Format Notice")
        st.info(
            """âš ï¸ **Important:**  
            - Your preprocessed dataset must match the model's trained feature structure.  
            - Unknown categories or missing dummy columns will be automatically filled with zeros.  
            - Make sure preprocessing steps like encoding and outlier removal were applied."""
        )

        model, scaler = load_model_and_scaler()
        if model is None or scaler is None:
            st.error("ðŸš« Model or Scaler files not found. Make sure 'fraud_model.pkl' and 'scaler.pkl' are in the working directory.")
            st.stop()

        # Align input dataset with model features
        model_features = model.feature_names_in_
        for col in model_features:
            if col not in dataset.columns:
                dataset[col] = 0  # Add missing columns as 0
        dataset = dataset[model_features]  # Reorder columns

        # Apply scaling and prediction
        scaled_data = scaler.transform(dataset)
        predictions = model.predict(scaled_data)
        probabilities = model.predict_proba(scaled_data)[:, 1] * 100

        # Attach results to the dataset
        result_dataset = dataset.copy()
        result_dataset["Fraud Prediction"] = predictions.astype(int)
        result_dataset["Fraud Probability (%)"] = probabilities.round(2)

        st.subheader("ðŸ” Prediction Results")

        # Add checkbox to control how many rows to show
        if st.checkbox("Show all prediction results", value=False):
            st.dataframe(result_dataset[["Fraud Prediction", "Fraud Probability (%)"]])
        else:
            st.dataframe(result_dataset[["Fraud Prediction", "Fraud Probability (%)"]].head(20))
        
        st.markdown("---")
        st.download_button("Download Predictions", result_dataset.to_csv(index=False), file_name="fraud_predictions.csv", mime="text/csv")
        st.success(f"âœ… Total transactions analyzed: {len(result_dataset)}")
        st.info(f"ðŸš¨ Fraudulent transactions detected: {(result_dataset['Fraud Prediction'] == 1).sum()}")

# Run the app
if __name__ == "__main__":
    streamlit_app()
>>>>>>> 2c3a8d0 (Deleted the foldersFlatten project structure for Streamlit Cloud compatibility):Fraud-Detection/app.py
